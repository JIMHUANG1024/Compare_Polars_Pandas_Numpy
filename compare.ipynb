{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10ec540e-a037-4346-bb27-a5e6597528d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from collections import Counter\n",
    "import time\n",
    "import polars as pl\n",
    "from pympler import asizeof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18613ae4-dd0c-4979-ada5-00c67e6e43e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Object Size of object: 3168521504 bytes\n",
      "Polars Object Size of object: 936 bytes\n"
     ]
    }
   ],
   "source": [
    "def generate_data(size):\n",
    "    if isinstance(size, list):\n",
    "        pdls, plls, npls = [], [], []\n",
    "        for s in size:\n",
    "            data = pd.DataFrame({\n",
    "                    'numerical_0': np.random.rand(s),\n",
    "                    'numerical_1': np.random.rand(s),\n",
    "                    'category_0': np.random.randint(5, size=s),\n",
    "                    'category_1': np.random.randint(8, size=s),\n",
    "                })\n",
    "            pdls.append(data)\n",
    "            plls.append(pl.from_dataframe(data))\n",
    "            npls.append(data.values)\n",
    "    return pdls, plls, npls\n",
    "size = [10000, 1000000, 5000000, 10000000, 50000000]\n",
    "pdls, plls, npls = generate_data(size)\n",
    "category_name = ['category_0', 'category_1']\n",
    "numerical_name = ['numerical_0', 'numerical_1']\n",
    "\n",
    "size = asizeof.asizeof(pdls)\n",
    "print(f\"Pandas Object Size of object: {size} bytes\")\n",
    "size = asizeof.asizeof(plls)\n",
    "print(f\"Polars Object Size of object: {size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f379c-722b-4e73-847b-82078582ca62",
   "metadata": {},
   "source": [
    "## **Single Feature Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f73a1-08d7-434a-a4ef-f71b8c707666",
   "metadata": {},
   "source": [
    "#### **1. Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eecdf8f-53ab-4d63-8787-370bce63917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.002215862274169922\n",
      "1000000 Sample Execution time： 0.03449082374572754\n",
      "5000000 Sample Execution time： 0.17963290214538574\n",
      "10000000 Sample Execution time： 0.3504457473754883\n",
      "50000000 Sample Execution time： 1.621446132659912\n"
     ]
    }
   ],
   "source": [
    "### pandas\n",
    "for data in pdls:\n",
    "    start_time = time.time()\n",
    "    temp = data[category_name[0]].value_counts()\n",
    "    x = data.merge(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f40053-8b58-4ddc-8da0-0c63c35ca9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0020034313201904297\n",
      "1000000 Sample Execution time： 0.009187459945678711\n",
      "5000000 Sample Execution time： 0.04319405555725098\n",
      "10000000 Sample Execution time： 0.08982324600219727\n",
      "50000000 Sample Execution time： 0.4042835235595703\n"
     ]
    }
   ],
   "source": [
    "### polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.group_by(category_name[0]).agg([\n",
    "        pl.col(category_name[0]).count().alias(\"counts\")\n",
    "    ])\n",
    "    _ = data.join(temp, on=category_name[0], how=\"left\")\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9866523d-3e6b-4995-b59a-cf82039a9c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.013328075408935547\n",
      "1000000 Sample Execution time： 0.9126410484313965\n",
      "5000000 Sample Execution time： 4.73424220085144\n",
      "10000000 Sample Execution time： 9.089803457260132\n",
      "50000000 Sample Execution time： 43.22467517852783\n"
     ]
    }
   ],
   "source": [
    "### numpy\n",
    "for data in npls:\n",
    "    start_time = time.time()\n",
    "    temp = Counter(data[:, 2])\n",
    "    result = np.zeros_like(data[:, 2])\n",
    "    for i, val in enumerate(data[:, 2]):\n",
    "        if not np.isnan(val):\n",
    "            if isinstance(val, list) or isinstance(val, np.ndarray): val = val[0]\n",
    "            result[i] = temp[val]\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b9595-5fc5-4e63-9186-ee1b4b3539bb",
   "metadata": {},
   "source": [
    "#### **2. Delay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d152fb-3edf-4eb2-819f-f561f3243e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0\n",
      "1000000 Sample Execution time： 0.0020036697387695312\n",
      "5000000 Sample Execution time： 0.006906747817993164\n",
      "10000000 Sample Execution time： 0.01806044578552246\n",
      "50000000 Sample Execution time： 0.04733538627624512\n"
     ]
    }
   ],
   "source": [
    "### pandas\n",
    "for data in pdls:\n",
    "    start_time = time.time()\n",
    "    temp = data[numerical_name[0]].shift(10)\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3785bbc5-c4ff-4fad-ae0e-01807fddc263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.012961387634277344\n",
      "1000000 Sample Execution time： 0.0\n",
      "5000000 Sample Execution time： 0.0\n",
      "10000000 Sample Execution time： 0.0\n",
      "50000000 Sample Execution time： 0.0\n"
     ]
    }
   ],
   "source": [
    "### polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].shift(10).alias(\"shift\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08e231f-870d-40f7-955f-650023ee5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0005056858062744141\n",
      "1000000 Sample Execution time： 0.0025081634521484375\n",
      "5000000 Sample Execution time： 0.011437416076660156\n",
      "10000000 Sample Execution time： 0.02332019805908203\n",
      "50000000 Sample Execution time： 0.08516168594360352\n"
     ]
    }
   ],
   "source": [
    "### numpy\n",
    "for data in npls:\n",
    "    start_time = time.time()\n",
    "    delayed_col = np.roll(data[:, 0], 10)\n",
    "    delayed_col = delayed_col.astype(float)\n",
    "    delayed_col[:10] = np.nan  \n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878119c8-c99a-471f-bbb5-e88da6e122fd",
   "metadata": {},
   "source": [
    "#### **3. TSUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72de5a05-bbdb-421d-b949-69d45b310061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0012521743774414062\n",
      "1000000 Sample Execution time： 0.012411117553710938\n",
      "5000000 Sample Execution time： 0.056128501892089844\n",
      "10000000 Sample Execution time： 0.11829137802124023\n",
      "50000000 Sample Execution time： 0.6118054389953613\n"
     ]
    }
   ],
   "source": [
    "### pandas\n",
    "for data in pdls:\n",
    "    start_time = time.time()\n",
    "    temp = data[numerical_name[0]].rolling(10).sum()\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956b51b9-2d78-4d9f-9879-65c7c853cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.013611078262329102\n",
      "1000000 Sample Execution time： 0.003021717071533203\n",
      "5000000 Sample Execution time： 0.022639989852905273\n",
      "10000000 Sample Execution time： 0.06169581413269043\n",
      "50000000 Sample Execution time： 0.3012354373931885\n"
     ]
    }
   ],
   "source": [
    "### polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_sum(10).alias(\"tsum\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240163f4-98ac-4af7-bfd9-b3f170309b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.022855520248413086\n",
      "1000000 Sample Execution time： 2.1963748931884766\n",
      "5000000 Sample Execution time： 10.922600984573364\n",
      "10000000 Sample Execution time： 21.817306518554688\n",
      "50000000 Sample Execution time： 108.69869565963745\n"
     ]
    }
   ],
   "source": [
    "### numpy\n",
    "def rolling_sum(x, window):\n",
    "    sum_arr = np.full_like(x, np.nan, dtype=np.float64)\n",
    "    for i in range(window - 1, len(x)):\n",
    "        sum_arr[i] = np.sum(x[i - window + 1:i + 1])\n",
    "    return sum_arr\n",
    "for data in npls:\n",
    "    start_time = time.time()\n",
    "    rolling_sum(data[:, 0], 10)\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6eb9c9-eee5-4c1c-9672-814cb393f567",
   "metadata": {},
   "source": [
    "## Intermediate Conclusion\n",
    "\n",
    "- **Observation 1**: Numpy hasn`t plenty functions to transformer data.\n",
    "- **Observation 2**: Compared to pandas or polars, the calculate time is can`t tolerate, so abandon numpy.\n",
    "- **Observation 3**: Polars is faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c4b59-bb54-4060-848e-21edaa6ee414",
   "metadata": {},
   "source": [
    "#### **4. TMAX, TMEAN, TMEDIAN, TMIN, TQUANTILE, TSKEW, TSTD, TVAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad79cd-9047-4d31-b931-d1fe8ddb2e92",
   "metadata": {},
   "source": [
    "#### According to Observation above, just test for the time series operators polars is faster than pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a6d084-81db-4d3f-a93e-a9fb9440c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pandas\n",
    "pandas_spend_times = []\n",
    "for data in pdls:\n",
    "    # tmax\n",
    "    start_time = time.time()\n",
    "    data['tmax'] = data[numerical_name[0]].rolling(10).max()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tmean\n",
    "    start_time = time.time()\n",
    "    data['tmean'] = data[numerical_name[0]].rolling(10).mean()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tmedian\n",
    "    start_time = time.time()\n",
    "    data['tmedian'] = data[numerical_name[0]].rolling(10).median()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tmin\n",
    "    start_time = time.time()\n",
    "    data['tmin'] = data[numerical_name[0]].rolling(10).min()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tquantile\n",
    "    start_time = time.time()\n",
    "    data['tquantile'] = data[numerical_name[0]].rolling(10).quantile(q=0.5)\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tskew\n",
    "    start_time = time.time()\n",
    "    data['tskew'] = data[numerical_name[0]].rolling(10).skew()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tstd\n",
    "    start_time = time.time()\n",
    "    data['tstd'] = data[numerical_name[0]].rolling(10).std()\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # tvar\n",
    "    start_time = time.time()\n",
    "    data['tvar'] = data[numerical_name[0]].rolling(10).var()\n",
    "    pandas_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9866636d-214e-4ca2-bc16-9c792723f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### polars\n",
    "polars_spend_times = []\n",
    "for data in plls:\n",
    "    # tmax\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_max(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tmean\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_mean(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tmedian\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_median(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tmin\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_min(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tquantile\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_quantile(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tskew\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_skew(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tstd\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_std(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # tvar\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_var(10).alias(\"tsum\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e66313-64a9-4f44-9fec-820789d1bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Samples TMAX  Execution time difference:\t -0.01311182975769043\n",
      "10000 Samples TMEAN  Execution time difference:\t 0.0\n",
      "10000 Samples TMEDIAN  Execution time difference:\t 0.0020508766174316406\n",
      "10000 Samples TMIN  Execution time difference:\t 0.0\n",
      "10000 Samples TQUANTILE  Execution time difference:\t 0.001016378402709961\n",
      "10000 Samples TSKEW  Execution time difference:\t -0.0117340087890625\n",
      "10000 Samples TSTD  Execution time difference:\t 0.0010104179382324219\n",
      "10000 Samples TVAR  Execution time difference:\t 0.0\n",
      "1000000 Samples TMAX  Execution time difference:\t 0.011887311935424805\n",
      "1000000 Samples TMEAN  Execution time difference:\t 0.005140066146850586\n",
      "1000000 Samples TMEDIAN  Execution time difference:\t 0.12295031547546387\n",
      "1000000 Samples TMIN  Execution time difference:\t 0.009814023971557617\n",
      "1000000 Samples TQUANTILE  Execution time difference:\t 0.160841703414917\n",
      "1000000 Samples TSKEW  Execution time difference:\t -1.1319470405578613\n",
      "1000000 Samples TSTD  Execution time difference:\t 0.007920265197753906\n",
      "1000000 Samples TVAR  Execution time difference:\t 0.009032726287841797\n",
      "5000000 Samples TMAX  Execution time difference:\t 0.054369211196899414\n",
      "5000000 Samples TMEAN  Execution time difference:\t 0.04309344291687012\n",
      "5000000 Samples TMEDIAN  Execution time difference:\t 0.6473789215087891\n",
      "5000000 Samples TMIN  Execution time difference:\t 0.057924509048461914\n",
      "5000000 Samples TQUANTILE  Execution time difference:\t 0.8502933979034424\n",
      "5000000 Samples TSKEW  Execution time difference:\t -4.51440954208374\n",
      "5000000 Samples TSTD  Execution time difference:\t 0.05218911170959473\n",
      "5000000 Samples TVAR  Execution time difference:\t 0.046921491622924805\n",
      "10000000 Samples TMAX  Execution time difference:\t 0.1170654296875\n",
      "10000000 Samples TMEAN  Execution time difference:\t 0.09072303771972656\n",
      "10000000 Samples TMEDIAN  Execution time difference:\t 1.416383981704712\n",
      "10000000 Samples TMIN  Execution time difference:\t 0.1194915771484375\n",
      "10000000 Samples TQUANTILE  Execution time difference:\t 1.7001945972442627\n",
      "10000000 Samples TSKEW  Execution time difference:\t -9.045674085617065\n",
      "10000000 Samples TSTD  Execution time difference:\t 0.11552739143371582\n",
      "10000000 Samples TVAR  Execution time difference:\t 0.0899040699005127\n",
      "50000000 Samples TMAX  Execution time difference:\t 0.5288586616516113\n",
      "50000000 Samples TMEAN  Execution time difference:\t 0.3633871078491211\n",
      "50000000 Samples TMEDIAN  Execution time difference:\t 6.884552717208862\n",
      "50000000 Samples TMIN  Execution time difference:\t 0.6071872711181641\n",
      "50000000 Samples TQUANTILE  Execution time difference:\t 8.431092977523804\n",
      "50000000 Samples TSKEW  Execution time difference:\t -45.1544451713562\n",
      "50000000 Samples TSTD  Execution time difference:\t 0.3896768093109131\n",
      "50000000 Samples TVAR  Execution time difference:\t 0.5933308601379395\n"
     ]
    }
   ],
   "source": [
    "op_name = ['TMAX', 'TMEAN', 'TMEDIAN', 'TMIN', 'TQUANTILE', 'TSKEW', 'TSTD', 'TVAR'] * 5\n",
    "for i, (a, b) in enumerate(zip(pandas_spend_times, polars_spend_times)):\n",
    "    print(f\"{size[i//8]} Samples\", op_name[i % len(op_name)], \" Execution time difference:\\t\", a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c733a1-1277-4660-b423-534cb6d76994",
   "metadata": {},
   "source": [
    "### **Interesting thing is rolling_skew polars is slower than pandas**\n",
    "\n",
    "- One reason is polars and pandas the result is different.\n",
    "- We try the Mathematically defined skew by using numpy and scripy, the result is same with the polars.\n",
    "\n",
    "#### This is a example in pandas\n",
    "```\n",
    "ser = pd.Series([1, 5, 2, 7, 15, 6])\n",
    "ser.rolling(3).skew().round(6)\n",
    "0         NaN\n",
    "1         NaN\n",
    "2    1.293343\n",
    "3   -0.585583\n",
    "4    0.670284\n",
    "5    1.652317\n",
    "dtype: float64\n",
    "```\n",
    "**skew([1,5,2]) should be 0.5280, but the results is 1.293343, we don`t check the pandas resource code, for Feature Engingeer, we perfer polars result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b9a19d-be59-4f44-bf31-b1624bbdd634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9906771678583391"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdls[0][numerical_name[0]].rolling(10).skew()[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba6e06c-5c95-435d-ae5f-3a76032d2e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tskew</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.835412</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌──────────┐\n",
       "│ tskew    │\n",
       "│ ---      │\n",
       "│ f64      │\n",
       "╞══════════╡\n",
       "│ 0.835412 │\n",
       "└──────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plls[0].select([plls[0][numerical_name[0]].rolling_skew(10).alias(\"tskew\")])[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ffdc3a-91d8-4a42-9d79-dfa974676ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8354123403619467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_skewness(data):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=0)\n",
    "    \n",
    "    skewness = (np.sum((data - mean) ** 3) / n) / (std_dev ** 3)\n",
    "    \n",
    "    return skewness\n",
    "x = pdls[0][numerical_name[0]].values[:10]\n",
    "calculate_skewness(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3710bdab-c5b4-4f8f-a3f1-98afb50d8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280049792181881"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "skew(np.array([1, 5, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e15810d-20ba-4232-9b88-f6ddf611fe5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.528004979218188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_skewness(np.array([1, 5, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c54e0-16e7-4378-adfc-14aec8fcdae0",
   "metadata": {},
   "source": [
    "#### **5. DELTA, TSARGMAX, TSARGMIN, STDDEV, DECAYLINEAR**\n",
    "the operate is no built-in functions both in pandas and polars\n",
    "\n",
    "polars execute take lots of time, we don`t run it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59570e0f-6279-468c-bf78-bc6e65a524b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 1.0883605480194092\n",
      "1000000 Sample Execution time： 0.2853522300720215\n",
      "5000000 Sample Execution time： 0.557382345199585\n",
      "10000000 Sample Execution time： 0.8201844692230225\n",
      "50000000 Sample Execution time： 3.512913703918457\n"
     ]
    }
   ],
   "source": [
    "### pandas\n",
    "for data in pdls:\n",
    "    start_time = time.time()\n",
    "    temp = data[numerical_name[0]].rolling(10).apply(lambda x: x.argmin(), raw=True, engine='numba')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b188ab1f-4188-4f21-82d4-6d6761bc874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.08661174774169922\n",
      "1000000 Sample Execution time： 6.907378673553467\n",
      "5000000 Sample Execution time： 23.615180253982544\n",
      "10000000 Sample Execution time： 46.79049873352051\n",
      "50000000 Sample Execution time： 236.20098090171814\n"
     ]
    }
   ],
   "source": [
    "### polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_map(\n",
    "        window_size=10, function=lambda x: int(pl.Series(x).arg_max())).alias(\"rolling_argmax\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4605e0ab-8537-4afa-b6e8-db38eab74a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.12421202659606934\n",
      "1000000 Sample Execution time： 5.395846605300903\n",
      "5000000 Sample Execution time： 26.17410135269165\n",
      "10000000 Sample Execution time： 52.41760063171387\n",
      "50000000 Sample Execution time： 263.3735523223877\n"
     ]
    }
   ],
   "source": [
    "### polars\n",
    "weights = np.linspace(0.1, 1, int(10))\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        data[numerical_name[0]].rolling_map(\n",
    "        window_size=10, function=lambda x: (x * weights).sum()).alias(\"DECAYLINEAR\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054f587-ed41-47e3-a656-5b78015f649e",
   "metadata": {},
   "source": [
    "### **Interesting thing is apply or map function polars is slower than pandas,**\n",
    "### **on the other hand pandas+numba is better than polars for this situation.**\n",
    "#### **why!**\n",
    "#### **The ChatGPT`s explain is:**\n",
    "1. **Numba Acceleration:** Pandas' rolling operation, combined with Numba, allows for just-in-time compilation of functions, which can significantly speed up computations with large datasets. Numba often performs faster than libraries built on interpreted languages (like Python or Rust's higher-level libraries) when handling numerical calculations.\n",
    "\n",
    "2. **Polars Internal Implementation:** Polars' rolling_map function might not have the same just-in-time compilation capabilities as Numba. While Polars is based on Rust and generally performs excellently, in this specific case (using custom functions), it might not optimize the operation as effectively as Numba.\n",
    "\n",
    "3. **Efficiency of Rolling Operations:** Different libraries implement rolling operations with varying underlying logic, which can lead to differences in efficiency. For instance, Polars may have more complex or additional steps when handling rolling_map, which could slow down the process compared to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71afb2b-6976-4730-aadf-5f03adc4f5e1",
   "metadata": {},
   "source": [
    "#### **6.Sine, Cosine, Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf7dea13-39dd-4f70-9704-c60a4f75a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pandas\n",
    "pandas_spend_times = []\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # 对输入值进行缩放以避免数值不稳定性\n",
    "    return e_x / e_x.sum()\n",
    "for data in pdls:\n",
    "    # sine\n",
    "    start_time = time.time()\n",
    "    data['sin'] = np.sin(data[numerical_name[0]])\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # cos\n",
    "    start_time = time.time()\n",
    "    data['cos'] = np.cos(data[numerical_name[0]])\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # softmax\n",
    "    start_time = time.time()\n",
    "    data['softmax'] = softmax(data[numerical_name[0]].values)\n",
    "    pandas_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e231156e-e4c2-475a-85bf-bf38ef21bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### polars\n",
    "polars_spend_times = []\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # 对输入值进行缩放以避免数值不稳定性\n",
    "    return e_x / e_x.sum()\n",
    "for data in plls:\n",
    "    # sine\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.col(numerical_name[0]).sin().alias('sine')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # cos\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.col(numerical_name[0]).sin().alias('cos')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # softmax\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.Series(softmax(data[numerical_name[0]].to_numpy())).alias(\"softmax_values\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daf79ce0-7655-45ec-966c-185840f8c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Samples Sine  Execution time difference:\t -0.016792774200439453\n",
      "10000 Samples Cosine  Execution time difference:\t 0.0\n",
      "10000 Samples Softmax  Execution time difference:\t -0.0010035037994384766\n",
      "10000 Samples Sine  Execution time difference:\t 0.003069639205932617\n",
      "10000 Samples Cosine  Execution time difference:\t -0.0009696483612060547\n",
      "10000 Samples Softmax  Execution time difference:\t 0.0010933876037597656\n",
      "10000 Samples Sine  Execution time difference:\t 0.00511932373046875\n",
      "10000 Samples Cosine  Execution time difference:\t 0.006043195724487305\n",
      "1000000 Samples Softmax  Execution time difference:\t 0.009743690490722656\n",
      "1000000 Samples Sine  Execution time difference:\t 0.008233070373535156\n",
      "1000000 Samples Cosine  Execution time difference:\t -0.0029811859130859375\n",
      "1000000 Samples Softmax  Execution time difference:\t 0.007904767990112305\n",
      "1000000 Samples Sine  Execution time difference:\t 0.04054903984069824\n",
      "1000000 Samples Cosine  Execution time difference:\t -0.08980989456176758\n",
      "1000000 Samples Softmax  Execution time difference:\t 0.054833412170410156\n"
     ]
    }
   ],
   "source": [
    "op_name = ['Sine', 'Cosine', 'Softmax']\n",
    "for i, (a, b) in enumerate(zip(pandas_spend_times, polars_spend_times)):\n",
    "    print(f\"{size[i//3]} Samples\", op_name[i % len(op_name)], \" Execution time difference:\\t\", a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abeb4d-2882-4e64-a39d-eaa7f3bfee2a",
   "metadata": {},
   "source": [
    "#### Polars is not better than pandas, that is interesting, but the running time is accepetable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b64929-8acd-4ea6-bb86-dbbcf2d5196e",
   "metadata": {},
   "source": [
    "## **Double Feature Transformer**\n",
    "'add', 'divide', 'multiply', 'subtract', 'bigger', 'smaller', 'equal', 'minimize', 'maximize', 'tcorvariance', 'tcorrelation', 'aggregate', 'crosscount', 'nunique'\n",
    "\n",
    "**Before testing, We feel the speed of operators that can be directly calculated will be very close. Let`s check and see the result of aggregate and rolling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83141295-9f06-480e-8f74-0b1200750ccc",
   "metadata": {},
   "source": [
    "#### **7.'add', 'divide', 'multiply', 'subtract', 'bigger', 'smaller', 'equal', 'minimize', 'maximize'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49d39efc-4924-44fb-97a3-3302a2af32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pandas\n",
    "pandas_spend_times = []\n",
    "for data in pdls:\n",
    "    # add\n",
    "    start_time = time.time()\n",
    "    data['add'] = data[numerical_name[0]] + data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # divide\n",
    "    start_time = time.time()\n",
    "    data['divide'] = data[numerical_name[0]] / data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # multiply\n",
    "    start_time = time.time()\n",
    "    data['multiply'] = data[numerical_name[0]] * data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # subtract\n",
    "    start_time = time.time()\n",
    "    data['subtract'] = data[numerical_name[0]] - data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # bigger\n",
    "    start_time = time.time()\n",
    "    data['bigger'] = data[numerical_name[0]] > data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # smaller\n",
    "    start_time = time.time()\n",
    "    data['smaller'] = data[numerical_name[0]] < data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # equal\n",
    "    start_time = time.time()\n",
    "    data['equal'] = data[numerical_name[0]] == data[numerical_name[1]]\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # minimize\n",
    "    start_time = time.time()\n",
    "    data['minimize'] = np.min([data[numerical_name[0]], data[numerical_name[1]]], axis=0)\n",
    "    pandas_spend_times.append(time.time() - start_time)\n",
    "    # maximize\n",
    "    start_time = time.time()\n",
    "    data['maximize'] = np.max([data[numerical_name[0]], data[numerical_name[1]]], axis=0)\n",
    "    pandas_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd916bb4-6054-4752-9dab-733ab334fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### polars\n",
    "polars_spend_times = []\n",
    "for data in plls:\n",
    "    # add\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) + pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # divide\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) / pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # multiply\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) * pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # subtract\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) - pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # bigger\n",
    "    start_time = time.time()\n",
    "    dtemp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) > pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # smaller\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) < pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # equal\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        (pl.col(numerical_name[0]) == pl.col(numerical_name[1])).alias('add')\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # minimize\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.when(pl.col(numerical_name[0]) < pl.col(numerical_name[1])).then(pl.col(numerical_name[0]))\n",
    "        .otherwise(pl.col(numerical_name[1]))\n",
    "        .alias(\"smller_value\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)\n",
    "    # maximize\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.when(pl.col(numerical_name[0]) > pl.col(numerical_name[1])).then(pl.col(numerical_name[0]))\n",
    "        .otherwise(pl.col(numerical_name[1]))\n",
    "        .alias(\"greater_value\")\n",
    "    ])\n",
    "    polars_spend_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b72a360-cea3-418a-aa8a-40330e22b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Samples add  Execution time difference:\t -0.0010025501251220703\n",
      "10000 Samples divide  Execution time difference:\t 0.0010051727294921875\n",
      "10000 Samples multiply  Execution time difference:\t 0.0\n",
      "10000 Samples subtract  Execution time difference:\t 0.0\n",
      "10000 Samples bigger  Execution time difference:\t 0.0\n",
      "10000 Samples smaller  Execution time difference:\t 0.0\n",
      "10000 Samples equal  Execution time difference:\t 0.0\n",
      "10000 Samples minimize  Execution time difference:\t -0.009007453918457031\n",
      "10000 Samples maximize  Execution time difference:\t 0.0\n",
      "1000000 Samples add  Execution time difference:\t -6.9141387939453125e-06\n",
      "1000000 Samples divide  Execution time difference:\t -0.0002696514129638672\n",
      "1000000 Samples multiply  Execution time difference:\t 0.002007007598876953\n",
      "1000000 Samples subtract  Execution time difference:\t 0.001003265380859375\n",
      "1000000 Samples bigger  Execution time difference:\t 0.0010037422180175781\n",
      "1000000 Samples smaller  Execution time difference:\t 0.001003265380859375\n",
      "1000000 Samples equal  Execution time difference:\t -9.5367431640625e-07\n",
      "1000000 Samples minimize  Execution time difference:\t 0.004452705383300781\n",
      "1000000 Samples maximize  Execution time difference:\t 0.003047943115234375\n",
      "5000000 Samples add  Execution time difference:\t 0.005696535110473633\n",
      "5000000 Samples divide  Execution time difference:\t -0.02165818214416504\n",
      "5000000 Samples multiply  Execution time difference:\t 0.006381988525390625\n",
      "5000000 Samples subtract  Execution time difference:\t 0.007948637008666992\n",
      "5000000 Samples bigger  Execution time difference:\t 0.001127481460571289\n",
      "5000000 Samples smaller  Execution time difference:\t 0.0011181831359863281\n",
      "5000000 Samples equal  Execution time difference:\t 2.6226043701171875e-06\n",
      "5000000 Samples minimize  Execution time difference:\t 0.01535344123840332\n",
      "5000000 Samples maximize  Execution time difference:\t 0.014048099517822266\n",
      "10000000 Samples add  Execution time difference:\t 0.009031057357788086\n",
      "10000000 Samples divide  Execution time difference:\t -0.005550861358642578\n",
      "10000000 Samples multiply  Execution time difference:\t 0.012042045593261719\n",
      "10000000 Samples subtract  Execution time difference:\t 0.013596773147583008\n",
      "10000000 Samples bigger  Execution time difference:\t 0.000820159912109375\n",
      "10000000 Samples smaller  Execution time difference:\t 0.002915620803833008\n",
      "10000000 Samples equal  Execution time difference:\t 0.0010082721710205078\n",
      "10000000 Samples minimize  Execution time difference:\t 0.03387618064880371\n",
      "10000000 Samples maximize  Execution time difference:\t 0.030809640884399414\n",
      "50000000 Samples add  Execution time difference:\t 0.059502601623535156\n",
      "50000000 Samples divide  Execution time difference:\t 0.01210331916809082\n",
      "50000000 Samples multiply  Execution time difference:\t 0.04372596740722656\n",
      "50000000 Samples subtract  Execution time difference:\t 0.21700739860534668\n",
      "50000000 Samples bigger  Execution time difference:\t 0.01168680191040039\n",
      "50000000 Samples smaller  Execution time difference:\t -0.01598644256591797\n",
      "50000000 Samples equal  Execution time difference:\t 0.009955883026123047\n",
      "50000000 Samples minimize  Execution time difference:\t 0.2792787551879883\n",
      "50000000 Samples maximize  Execution time difference:\t 0.1672980785369873\n"
     ]
    }
   ],
   "source": [
    "op_name = ['add', 'divide', 'multiply', 'subtract', 'bigger', 'smaller', 'equal', 'minimize', 'maximize']\n",
    "for i, (a, b) in enumerate(zip(pandas_spend_times, polars_spend_times)):\n",
    "    print(f\"{size[i//9]} Samples\", op_name[i % len(op_name)], \" Execution time difference:\\t\", a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44896466-7de8-4f01-87ca-ace472b3047e",
   "metadata": {},
   "source": [
    "#### **8.'tcorvariance', 'tcorrelation'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a1117d0-ffcc-4ab1-b7e7-fc38b63b9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0010640621185302734\n",
      "1000000 Sample Execution time： 0.08747148513793945\n",
      "5000000 Sample Execution time： 0.40174102783203125\n",
      "10000000 Sample Execution time： 0.8168542385101318\n",
      "50000000 Sample Execution time： 4.162893295288086\n"
     ]
    }
   ],
   "source": [
    "for data in pdls:\n",
    "    # tcorvariance\n",
    "    start_time = time.time()\n",
    "    data['tcorvariance'] = data[numerical_name[0]].rolling(10).corr(data[numerical_name[1]])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b35571c8-6600-4bbc-b4bf-6e7fdbd68823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0298616886138916\n",
      "1000000 Sample Execution time： 0.03975105285644531\n",
      "5000000 Sample Execution time： 0.20784521102905273\n",
      "10000000 Sample Execution time： 0.40465521812438965\n",
      "50000000 Sample Execution time： 2.06021785736084\n"
     ]
    }
   ],
   "source": [
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.rolling_corr(numerical_name[0], numerical_name[1], window_size=10).alias(\"tcorvariance\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22d50039-673d-49ea-b463-63440c6cda93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0010018348693847656\n",
      "1000000 Sample Execution time： 0.0770108699798584\n",
      "5000000 Sample Execution time： 0.270263671875\n",
      "10000000 Sample Execution time： 0.5352280139923096\n",
      "50000000 Sample Execution time： 2.7887940406799316\n"
     ]
    }
   ],
   "source": [
    "for data in pdls:\n",
    "    # tcorvariance\n",
    "    start_time = time.time()\n",
    "    data['tcorvariance'] = data[numerical_name[0]].rolling(10).cov(data[numerical_name[1]])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc5161d4-1a01-40a0-bce1-7017d815665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.0035583972930908203\n",
      "1000000 Sample Execution time： 0.026164770126342773\n",
      "5000000 Sample Execution time： 0.19764947891235352\n",
      "10000000 Sample Execution time： 0.3112671375274658\n",
      "50000000 Sample Execution time： 1.4020442962646484\n"
     ]
    }
   ],
   "source": [
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.with_columns([\n",
    "        pl.rolling_cov(numerical_name[0], numerical_name[1], window_size=10).alias(\"tcorvariance\")\n",
    "    ])\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f66e6-240a-408e-85ee-a92eea627dca",
   "metadata": {},
   "source": [
    "#### **9.'aggregate', 'crosscount', 'nunique'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bae3248f-62ef-4466-91b8-4585a4d8fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.39626193046569824\n",
      "1000000 Sample Execution time： 0.09432721138000488\n",
      "5000000 Sample Execution time： 0.48881030082702637\n",
      "10000000 Sample Execution time： 0.9089641571044922\n",
      "50000000 Sample Execution time： 5.315707683563232\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "for data in pdls:\n",
    "    # aggmean\n",
    "    start_time = time.time()\n",
    "    temp = data.groupby([category_name[0]])[numerical_name[0]].agg('sum').rename('aggmean')\n",
    "    _ = data.merge(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c99d9bf-8ac3-411b-8e40-ae48f558f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.027961015701293945\n",
      "1000000 Sample Execution time： 0.0075075626373291016\n",
      "5000000 Sample Execution time： 0.026412487030029297\n",
      "10000000 Sample Execution time： 0.056580305099487305\n",
      "50000000 Sample Execution time： 0.35039710998535156\n"
     ]
    }
   ],
   "source": [
    "# polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.group_by(category_name[0]).agg(pl.sum(numerical_name[0]).alias('aggmean'))\n",
    "    result = data.join(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ad4a4f1-6b8c-45e5-b43b-f2923933e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.014368057250976562\n",
      "1000000 Sample Execution time： 0.1665959358215332\n",
      "5000000 Sample Execution time： 1.256286859512329\n",
      "10000000 Sample Execution time： 2.826300859451294\n",
      "50000000 Sample Execution time： 15.91285490989685\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "for data in pdls:\n",
    "    # aggmean\n",
    "    start_time = time.time()\n",
    "    temp = data.groupby([category_name[0]])[numerical_name[0]].nunique().rename('nunique')\n",
    "    _ = data.merge(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a18ced26-25ce-4828-a8fb-a6f833351e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.018143177032470703\n",
      "1000000 Sample Execution time： 0.01438140869140625\n",
      "5000000 Sample Execution time： 0.07217621803283691\n",
      "10000000 Sample Execution time： 0.1534109115600586\n",
      "50000000 Sample Execution time： 0.6692502498626709\n"
     ]
    }
   ],
   "source": [
    "# polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.group_by(category_name[0]).agg(pl.col(numerical_name[0]).n_unique().alias('nunique'))\n",
    "    result = data.join(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6463968a-df6a-4ed5-9a9c-b2b85497eb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.33628177642822266\n",
      "1000000 Sample Execution time： 0.10019993782043457\n",
      "5000000 Sample Execution time： 0.47657275199890137\n",
      "10000000 Sample Execution time： 1.0682213306427002\n",
      "50000000 Sample Execution time： 5.361292362213135\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "for data in pdls:\n",
    "    # aggmean\n",
    "    start_time = time.time()\n",
    "    temp = data.groupby([category_name[0]])[numerical_name[0]].count().rename('nunique')\n",
    "    _ = data.merge(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cbb5f2bb-f798-4673-ba69-01e2e6b65717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Sample Execution time： 0.051223039627075195\n",
      "1000000 Sample Execution time： 0.008140087127685547\n",
      "5000000 Sample Execution time： 0.03714179992675781\n",
      "10000000 Sample Execution time： 0.10254335403442383\n",
      "50000000 Sample Execution time： 0.4471867084503174\n"
     ]
    }
   ],
   "source": [
    "# polars\n",
    "for data in plls:\n",
    "    start_time = time.time()\n",
    "    temp = data.group_by(category_name[0]).agg(pl.col(numerical_name[0]).count().alias('nunique'))\n",
    "    result = data.join(temp, on=category_name[0], how='left')\n",
    "    print(f\"{len(data)} Sample Execution time：\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7128f-82ae-4349-a476-48ac606b4aec",
   "metadata": {},
   "source": [
    "### **Just like we say before, the speed of operators that can be directly calculated is very close. For aggregate operators, polars is better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd72c5-7da3-4813-883d-66e3e01f1422",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- **Memory**: polars uses much less memory than pandas. If you have the memory problem, just use polars.\n",
    "- **Speed**: In fact, the operation speed of pandas and polars is similar, but polars does not have a ready-made interface for custom functions of rolling, and the speed is unacceptable. Thanks to numba, pandas' custom rolling functions are very fast. For aggregate, polars does not find a 'map' or 'apply' function to use custom functions. For other operations, polars is slightly faster than pandas overall.\n",
    "- **Conclusion**: You can use polars and Use pandas+numba for custom function. Except for custom conversion functions, using polars is the best solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea341610-1cdc-4430-af8f-99f794c141af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59844129-59f7-4c54-9fda-b3252577b142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9d19-bbc8-4453-8728-d0b9720cbd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
